{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tejasri-Pendota/Open-source-Practice/blob/main/train_using_pretrained_model_image_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1fb1c10e",
      "metadata": {
        "id": "1fb1c10e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mTeGpwIcyRTr",
        "outputId": "75eacfa2-21cf-487a-e5b0-685a6e0b1acd"
      },
      "id": "mTeGpwIcyRTr",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ba57e286-c226-4528-927d-dde973ebdace\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ba57e286-c226-4528-927d-dde973ebdace\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving helper_functions.py to helper_functions (1).py\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'helper_functions (1).py': b'\"\"\"\\nA series of helper functions used throughout the course.\\n\\nIf a function gets defined once and could be used over and over, it\\'ll go in here.\\n\"\"\"\\nimport torch\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfrom torch import nn\\nimport os\\nimport zipfile\\nfrom pathlib import Path\\nimport requests\\nimport os\\n\\n\\n\\n# Plot linear data or training and test and predictions (optional)\\ndef plot_predictions(\\n    train_data, train_labels, test_data, test_labels, predictions=None\\n):\\n    \"\"\"\\n  Plots linear training data and test data and compares predictions.\\n  \"\"\"\\n    plt.figure(figsize=(10, 7))\\n\\n    # Plot training data in blue\\n    plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\\n\\n    # Plot test data in green\\n    plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\\n\\n    if predictions is not None:\\n        # Plot the predictions in red (predictions were made on the test data)\\n        plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\\n\\n    # Show the legend\\n    plt.legend(prop={\"size\": 14})\\n\\n\\n# Calculate accuracy (a classification metric)\\ndef accuracy_fn(y_true, y_pred):\\n    \"\"\"Calculates accuracy between truth labels and predictions.\\n\\n    Args:\\n        y_true (torch.Tensor): Truth labels for predictions.\\n        y_pred (torch.Tensor): Predictions to be compared to predictions.\\n\\n    Returns:\\n        [torch.float]: Accuracy value between y_true and y_pred, e.g. 78.45\\n    \"\"\"\\n    correct = torch.eq(y_true, y_pred).sum().item()\\n    acc = (correct / len(y_pred)) * 100\\n    return acc\\n\\n\\ndef print_train_time(start, end, device=None):\\n    \"\"\"Prints difference between start and end time.\\n\\n    Args:\\n        start (float): Start time of computation (preferred in timeit format). \\n        end (float): End time of computation.\\n        device ([type], optional): Device that compute is running on. Defaults to None.\\n\\n    Returns:\\n        float: time between start and end in seconds (higher is longer).\\n    \"\"\"\\n    total_time = end - start\\n    print(f\"\\\\nTrain time on {device}: {total_time:.3f} seconds\")\\n    return total_time\\n\\n\\n# Plot loss curves of a model\\ndef plot_loss_curves(results):\\n    \"\"\"Plots training curves of a results dictionary.\\n\\n    Args:\\n        results (dict): dictionary containing list of values, e.g.\\n            {\"train_loss\": [...],\\n             \"train_acc\": [...],\\n             \"test_loss\": [...],\\n             \"test_acc\": [...]}\\n    \"\"\"\\n    loss = results[\"train_loss\"]\\n    test_loss = results[\"test_loss\"]\\n\\n    accuracy = results[\"train_acc\"]\\n    test_accuracy = results[\"test_acc\"]\\n\\n    epochs = range(len(results[\"train_loss\"]))\\n\\n    plt.figure(figsize=(15, 7))\\n\\n    # Plot loss\\n    plt.subplot(1, 2, 1)\\n    plt.plot(epochs, loss, label=\"train_loss\")\\n    plt.plot(epochs, test_loss, label=\"test_loss\")\\n    plt.title(\"Loss\")\\n    plt.xlabel(\"Epochs\")\\n    plt.legend()\\n\\n    # Plot accuracy\\n    plt.subplot(1, 2, 2)\\n    plt.plot(epochs, accuracy, label=\"train_accuracy\")\\n    plt.plot(epochs, test_accuracy, label=\"test_accuracy\")\\n    plt.title(\"Accuracy\")\\n    plt.xlabel(\"Epochs\")\\n    plt.legend()\\n\\n\\n# Pred and plot image function from notebook 04\\n# See creation: https://www.learnpytorch.io/04_pytorch_custom_datasets/#113-putting-custom-image-prediction-together-building-a-function\\nfrom typing import List\\nimport torchvision\\n\\n\\ndef pred_and_plot_image(\\n    model: torch.nn.Module,\\n    image_path: str,\\n    class_names: List[str] = None,\\n    transform=None,\\n    device: torch.device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\\n):\\n    \"\"\"Makes a prediction on a target image with a trained model and plots the image.\\n\\n    Args:\\n        model (torch.nn.Module): trained PyTorch image classification model.\\n        image_path (str): filepath to target image.\\n        class_names (List[str], optional): different class names for target image. Defaults to None.\\n        transform (_type_, optional): transform of target image. Defaults to None.\\n        device (torch.device, optional): target device to compute on. Defaults to \"cuda\" if torch.cuda.is_available() else \"cpu\".\\n    \\n    Returns:\\n        Matplotlib plot of target image and model prediction as title.\\n\\n    Example usage:\\n        pred_and_plot_image(model=model,\\n                            image=\"some_image.jpeg\",\\n                            class_names=[\"class_1\", \"class_2\", \"class_3\"],\\n                            transform=torchvision.transforms.ToTensor(),\\n                            device=device)\\n    \"\"\"\\n\\n    # 1. Load in image and convert the tensor values to float32\\n    target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)\\n\\n    # 2. Divide the image pixel values by 255 to get them between [0, 1]\\n    target_image = target_image / 255.0\\n\\n    # 3. Transform if necessary\\n    if transform:\\n        target_image = transform(target_image)\\n\\n    # 4. Make sure the model is on the target device\\n    model.to(device)\\n\\n    # 5. Turn on model evaluation mode and inference mode\\n    model.eval()\\n    with torch.inference_mode():\\n        # Add an extra dimension to the image\\n        target_image = target_image.unsqueeze(dim=0)\\n\\n        # Make a prediction on image with an extra dimension and send it to the target device\\n        target_image_pred = model(target_image.to(device))\\n\\n    # 6. Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\\n    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\\n\\n    # 7. Convert prediction probabilities -> prediction labels\\n    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\\n\\n    # 8. Plot the image alongside the prediction and prediction probability\\n    plt.imshow(\\n        target_image.squeeze().permute(1, 2, 0)\\n    )  # make sure it\\'s the right size for matplotlib\\n    if class_names:\\n        title = f\"Pred: {class_names[target_image_pred_label.cpu()]} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\\n    else:\\n        title = f\"Pred: {target_image_pred_label} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\\n    plt.title(title)\\n    plt.axis(False)\\n\\ndef set_seeds(seed: int=42):\\n    \"\"\"Sets random sets for torch operations.\\n\\n    Args:\\n        seed (int, optional): Random seed to set. Defaults to 42.\\n    \"\"\"\\n    # Set the seed for general torch operations\\n    torch.manual_seed(seed)\\n    # Set the seed for CUDA torch operations (ones that happen on the GPU)\\n    torch.cuda.manual_seed(seed)\\n\\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import set_seeds"
      ],
      "metadata": {
        "id": "YJQ2qbFOySwK"
      },
      "id": "YJQ2qbFOySwK",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "12e97c71",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "12e97c71",
        "outputId": "7d02ac9d-ecbc-44d8-eaa6-cb05078aadb4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0de25b1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0de25b1a",
        "outputId": "e9dbc80b-498a-4d96-9c25-f9a50c5e9886"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
            "100%|██████████| 330M/330M [00:02<00:00, 118MB/s] \n"
          ]
        }
      ],
      "source": [
        "# 1. Get pretrained weights for ViT-Base\n",
        "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
        "\n",
        "# 2. Setup a ViT model instance with pretrained weights\n",
        "pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n",
        "\n",
        "# 3. Freeze the base parameters\n",
        "for parameter in pretrained_vit.parameters():\n",
        "    parameter.requires_grad = False\n",
        "\n",
        "# 4. Change the classifier head\n",
        "class_names = ['daisy','dandelion']\n",
        "\n",
        "set_seeds()\n",
        "pretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n",
        "# pretrained_vit # uncomment for model output"
      ]
    },
    {
      "source": [
        "!pip install torchinfo"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnqZ-4rIJX_a",
        "outputId": "5573988b-0202-4d28-f927-9a5e8cc69c77"
      },
      "id": "SnqZ-4rIJX_a",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "e3feaa42",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3feaa42",
        "outputId": "3adf733f-8445-47e5-e99f-c51a02150721"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "============================================================================================================================================\n",
              "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
              "============================================================================================================================================\n",
              "VisionTransformer (VisionTransformer)                        [32, 3, 224, 224]    [32, 2]              768                  Partial\n",
              "├─Conv2d (conv_proj)                                         [32, 3, 224, 224]    [32, 768, 14, 14]    (590,592)            False\n",
              "├─Encoder (encoder)                                          [32, 197, 768]       [32, 197, 768]       151,296              False\n",
              "│    └─Dropout (dropout)                                     [32, 197, 768]       [32, 197, 768]       --                   --\n",
              "│    └─Sequential (layers)                                   [32, 197, 768]       [32, 197, 768]       --                   False\n",
              "│    │    └─EncoderBlock (encoder_layer_0)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_1)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_2)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_3)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_4)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_5)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_6)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_7)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_8)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_9)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_10)                  [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_11)                  [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
              "│    └─LayerNorm (ln)                                        [32, 197, 768]       [32, 197, 768]       (1,536)              False\n",
              "├─Linear (heads)                                             [32, 768]            [32, 2]              1,538                True\n",
              "============================================================================================================================================\n",
              "Total params: 85,800,194\n",
              "Trainable params: 1,538\n",
              "Non-trainable params: 85,798,656\n",
              "Total mult-adds (Units.GIGABYTES): 5.52\n",
              "============================================================================================================================================\n",
              "Input size (MB): 19.27\n",
              "Forward/backward pass size (MB): 3330.74\n",
              "Params size (MB): 229.20\n",
              "Estimated Total Size (MB): 3579.20\n",
              "============================================================================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# Print a summary using torchinfo (uncomment for actual output)\n",
        "summary(model=pretrained_vit,\n",
        "        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
        "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c73ec300",
      "metadata": {
        "id": "c73ec300"
      },
      "source": [
        "#### Notice how only the output layer is trainable, where as, all of the rest of the layers are untrainable (frozen)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac8cc699",
      "metadata": {
        "id": "ac8cc699"
      },
      "outputs": [],
      "source": [
        "# Setup directory paths to train and test images\n",
        "train_dir = 'D:/tranformer_env/L-3_image_classification_using_ViT/custom_dataset/train'\n",
        "test_dir = 'D:/tranformer_env/L-3_image_classification_using_ViT/custom_dataset/test'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91175306",
      "metadata": {
        "id": "91175306"
      },
      "source": [
        "Remember, if you're going to use a pretrained model, it's generally important to ensure your own custom data is transformed/formatted in the same way the data the original model was trained on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05aa777b",
      "metadata": {
        "id": "05aa777b"
      },
      "outputs": [],
      "source": [
        "# Get automatic transforms from pretrained ViT weights\n",
        "pretrained_vit_transforms = pretrained_vit_weights.transforms()\n",
        "print(pretrained_vit_transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "088971e6",
      "metadata": {
        "id": "088971e6"
      },
      "source": [
        "## And now we've got transforms ready, we can turn our images into DataLoaders using the create_dataloaders()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d49225b",
      "metadata": {
        "id": "5d49225b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "def create_dataloaders(\n",
        "    train_dir: str,\n",
        "    test_dir: str,\n",
        "    transform: transforms.Compose,\n",
        "    batch_size: int,\n",
        "    num_workers: int=NUM_WORKERS\n",
        "):\n",
        "\n",
        "  # Use ImageFolder to create dataset(s)\n",
        "  train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
        "  test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
        "\n",
        "  # Get class names\n",
        "  class_names = train_data.classes\n",
        "\n",
        "  # Turn images into data loaders\n",
        "  train_dataloader = DataLoader(\n",
        "      train_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=True,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "  test_dataloader = DataLoader(\n",
        "      test_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "\n",
        "  return train_dataloader, test_dataloader, class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9037c8a5",
      "metadata": {
        "id": "9037c8a5"
      },
      "outputs": [],
      "source": [
        "# Setup dataloaders\n",
        "train_dataloader_pretrained, test_dataloader_pretrained, class_names = create_dataloaders(train_dir=train_dir,\n",
        "                                                                                                     test_dir=test_dir,\n",
        "                                                                                                     transform=pretrained_vit_transforms,\n",
        "                                                                                                     batch_size=32) # Could increase if we had more samples, such as here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10c5ba74",
      "metadata": {
        "id": "10c5ba74"
      },
      "outputs": [],
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "# Create optimizer and loss function\n",
        "optimizer = torch.optim.Adam(params=pretrained_vit.parameters(),\n",
        "                             lr=1e-3)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the classifier head of the pretrained ViT feature extractor model\n",
        "set_seeds()\n",
        "pretrained_vit_results = engine.train(model=pretrained_vit,\n",
        "                                      train_dataloader=train_dataloader_pretrained,\n",
        "                                      test_dataloader=test_dataloader_pretrained,\n",
        "                                      optimizer=optimizer,\n",
        "                                      loss_fn=loss_fn,\n",
        "                                      epochs=10,\n",
        "                                      device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "945149b4",
      "metadata": {
        "id": "945149b4"
      },
      "source": [
        "pretrained ViT performed far better than our custom ViT model trained from scratch (in the same amount of time).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2aae16a8",
      "metadata": {
        "id": "2aae16a8"
      },
      "outputs": [],
      "source": [
        "# Plot the loss curves\n",
        "from helper_functions import plot_loss_curves\n",
        "\n",
        "plot_loss_curves(pretrained_vit_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79922fd1",
      "metadata": {
        "id": "79922fd1"
      },
      "source": [
        "## That's the power of transfer learning!\n",
        "\n",
        "We managed to get outstanding results with the same model architecture, except our custom implementation was trained from scratch (worse performance) and this feature extractor model has the power of pretrained weights from ImageNet behind it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ce4427b",
      "metadata": {
        "id": "6ce4427b"
      },
      "source": [
        "# Let's make Prediction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe89b4e9",
      "metadata": {
        "id": "fe89b4e9"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# Import function to make predictions on images and plot them\n",
        "from going_modular.going_modular.predictions import pred_and_plot_image\n",
        "\n",
        "# Setup custom image path\n",
        "custom_image_path = \"test_img.jpg\"\n",
        "\n",
        "# Predict on custom image\n",
        "pred_and_plot_image(model=pretrained_vit,\n",
        "                    image_path=custom_image_path,\n",
        "                    class_names=class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e7d64d0",
      "metadata": {
        "id": "5e7d64d0"
      },
      "outputs": [],
      "source": [
        "# Import function to make predictions on images and plot them\n",
        "from going_modular.going_modular.predictions import pred_and_plot_image\n",
        "\n",
        "# Setup custom image path\n",
        "custom_image_path = \"test_1.jpg\"\n",
        "\n",
        "# Predict on custom image\n",
        "pred_and_plot_image(model=pretrained_vit,\n",
        "                    image_path=custom_image_path,\n",
        "                    class_names=class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb7d0f9e",
      "metadata": {
        "id": "eb7d0f9e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}